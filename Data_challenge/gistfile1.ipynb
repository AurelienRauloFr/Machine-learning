{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cbc0862",
   "metadata": {},
   "source": [
    "# Data Challenge : Introduction to Machine Learning (BGDIA703)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2910fcf",
   "metadata": {},
   "source": [
    "### Authors: \n",
    "#### Charlotte Laclau (charlotte.laclau@telecom-paris.fr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbfe27a",
   "metadata": {},
   "source": [
    "This year's challenge is about text classification. \n",
    "For privacy reasons, you are only provided with the embedding learned on the original documents. \n",
    "\n",
    "### Fair document classification\n",
    "\n",
    "The task is straightforward: assign the correct category to a text. This is thus a multi-class classification task with 28 classes to choose from.\n",
    "\n",
    "The most adopted paradigm consists in training a network $f: \\mathcal{X} \\rightarrow \\mathbb{R}^d$ which, from a given document $x \\in \\mathcal{X}$, extracts a feature vector $z \\in \\mathbb{R}^d$ which synthetizes the relevant caracteristics of $doc$. The diagnostic phase then consists, from an document $x$, to predict the label of the document based on the extracted features $z$. In this data challenge $d=768$. \n",
    "\n",
    "We directly provide you the embedding of each text (learned with BERT). \n",
    "\n",
    "The goal of this competition is to design a solution that is both accurate for predicting the label as well as fair with respect to some sensitive attribute (e.g. gender). Fairness in this context means that the model should not be biased toward a certain minority group present in the data. We explain this paradigm further in the evaluation part. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d08f529b",
   "metadata": {},
   "source": [
    "### Downloading the Data: \n",
    "\n",
    "You can download the Dataset and evaluator script from the below links: \n",
    "**https://partage.imt.fr/index.php/s/3M3Mn3sN3TNSDEp**\n",
    "\n",
    "**https://partage.imt.fr/index.php/s/CfajSjkAiq2oCLF**\n",
    "\n",
    "After unzipping the file, you have one pickle file named ``data-challenge-student.pickle``.\n",
    "In this file you have one dictionnary that contains the training set `X_train`, the test set `X_test`, the training label `Y_train`, and the information about the sensitive attribute in `S_train` and `S_test`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51769ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "with open('data-challenge-student.pickle', 'rb') as handle:\n",
    "    # dat = pickle.load(handle)\n",
    "    dat = pd.read_pickle(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe9c8206",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dat['X_train']\n",
    "Y = dat['Y']\n",
    "S = dat['S_train']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7e8be3",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "First of all, the accuracy of the solutions are evaluated according to the Macro F1 metric, The Macro F1 score is simply the arithmetic average of the F1 score for each class.\n",
    "\n",
    "We will also analyse proposed solutions according to their fairness with respect to the provided sensitive attribute ($S$). In other words, we want you to design a solution that is not biased towards one group in particular. To be specific, we will use (1-equal opportunity gap) between protected groups. A fair model is a model where this criteria is close to 1. \n",
    "\n",
    "Overall, your model should satisfy both criteria so the evaluation metric is the average between the macro F1 and the fairness criteria.\n",
    "\n",
    "The file `evaluator.py` contains the required functions to compute the final score on which you will be ranked. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada174d5",
   "metadata": {},
   "source": [
    "### Baseline \n",
    "\n",
    "Let us use a logistic regression as our naive baseline model. Note that this model does not take into accout the sensitive attribute $S$. It will only be used for the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b4d15f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from evaluator import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "add7acd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the logistic regression\n",
    "X_train, X_test, Y_train, Y_test, S_train, S_test = train_test_split(X, Y, S, test_size=0.3, random_state=42)\n",
    "clf = LogisticRegression(random_state=0, max_iter=5000).fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3434e1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = clf.predict(X_test)\n",
    "eval_scores, confusion_matrices_eval = gap_eval_scores(Y_pred, Y_test, S_test, metrics=['TPR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "486dc1bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7289043899708004\n"
     ]
    }
   ],
   "source": [
    "final_score = (eval_scores['macro_fscore']+ (1-eval_scores['TPR_GAP']))/2\n",
    "print(final_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f68cbc0",
   "metadata": {},
   "source": [
    "### Preparing the submission file\n",
    "\n",
    "Now we are ready to prepare a submission file. In the pickle you have access to some additional test data (`X_test`, `S_test`) and you should submit your prediction for `Y`.\n",
    "Note that with the current model, you do not need `S_test` but we provide it to you in case you want to use it in a debiasing strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1def5f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the \"true\" test data\n",
    "X_test = dat['X_test']\n",
    "S_test = dat['S_test'] \n",
    "# Classify the provided test data with you classifier\n",
    "y_test = clf.predict(X_test)\n",
    "results=pd.DataFrame(y_test, columns= ['score'])\n",
    "\n",
    "results.to_csv(\"Data_Challenge_MDI_341.csv\", header = None, index = None)\n",
    "# np.savetxt('y_test_challenge_student.txt', y_test, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc6cf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good luck !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}